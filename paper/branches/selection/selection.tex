% -*- mode: TeX-PDF-mode -*-
\documentclass[article]{jss}
\usepackage{amsmath}
\usepackage{bbm}

% ------- a few definitions ---------
\newcommand{\dnorm}{\phi}
\newcommand{\idiv}{\mathtt{//}}% integer division
\newcommand{\Natural}{\mathbb{N}}% Natural numbers.  needs ams-fonts
\newcommand{\pderiv}[2][]{\frac{\partial #1}{\partial #2}}
\newcommand{\Real}{\mathbb{R}}% Real numbers.  needs ams-fonts
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\pnorm}{\Phi}
\newcommand{\var}{\mathrm{Var}\,}
\newcommand{\corr}{\mathrm{Corr}}
\DeclareMathOperator*{\Exp}{\mathbbm{E}}% expectation
%\newcommand{\Exp}[1][]{\mathrm{E}_{#1}}
\newcommand{\indic}{\mathbbm{1}}% indicator function
\newcommand{\laplace}{\mathcal{L}}% Laplace transform
\newcommand{\lik}{\mathcal{L}}% likelihood
\newcommand{\loglik}{\ell}% log likelihood
\newcommand*{\mat}[1]{\mathsf{#1}}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}
\newcommand{\dif}{\mathrm{d}} % diferentsiaalimärk
\newcommand{\me}{\mathrm{e}} % Konstant e=2,71828
% -----------------------

\author{Arne Henningsen\\Kiel University
  \And
  Ott Toomet\\Tartu University}
\title{Sample Selection Models in \proglang{R}:\\
  Package \pkg{micEcon}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Arne Henningsen, Ott Toomet} %% comma-separated
\Plaintitle{Sample Selection Models in R: Package micEcon} %% without formatting
\Shorttitle{Sample Selection Models in R} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  Heckman-style sample selection models, and their implementation in package
  \pkg{micEcon} for \proglang{R}, are discussed.
}
\Keywords{sample-selection models, Heckman selection models, econometrics, \proglang{R}}
\Plainkeywords{sample-selection models, Heckman selection models, econometrics, R}

%% publication information
%% NOTE: This needs to filled out ONLY IF THE PAPER WAS ACCEPTED.
%% If it was not (yet) accepted, leave them commented.
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Ott Toomet\\
  Department of Economics\\
  Tartu University\\
  Narva 4-A123\\
  Tartu 51009, Estonia\\
  Telephone: +372 737 6348\\
  E-mail: \email{otoomet@ut.ee}\\
  URL: \url{http://www.obs.ee/~siim/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Sample selection}


\subsection{The problem}

Social scientists are often interested in causal effects -- what is
the impact of a new drug, a certain type of school or eg being born as
a twin.  Many of these cases are under our (partial) control.  In most
cases, we can decide ourselves, whether we take the drug or which
school we attend.  We cannot control whether we are twins, but neither
can the reasearcher -- if the twins tend to be born in a different
types of families, the researcher never has all the relevant
information in order to control for the difference.

This problem -- people who are ``treated'' are different that the rest
of the population -- is usually refered to as the \emph{sample
  selection} problem.  We cannot estimate the causal effect, unless we
solve the selection problem.  Otherwise, we never know which part of
the outcome is related to the fact that different people were selected
to the treatment and control groups, and which part is due to the
causal relationship.


\subsection{Possible solutions}

All the solutions involve using additional information.  There are
several possibilites which may or may not be useful for any particular
case. 

\begin{itemize}
\item Randomisation: experiment, where the participants \emph{do not}
  have control over their treatment status.  Although easy to analyse,
  this way is seldom feasible for pracitcal and ethical reasons.
\item Instruments (exclusion restrictions): more-or-less
  randomisation.  Just not explicit, based on background, institutions
  etc.
\item Information about the functional form.  For instance,
  assumptions about the distribution of the error terms.
\item Timing information.  In some cases, relative timing of the
  events may give us information, necessary for identification of the
  causal effect.
\end{itemize}

During the recent decades, either randomisation or the
pseudo-randimisation (natural experiments) has become the state-of-the
art while estimating the causal effects.  The methods, relying on the
distributional assumptions are becoming less widely used.  The reason
is obvious -- the parametric assumptions can only seldom be justified,
and we don't want our results to rely on dubious assumptions.  Even if
we have valid instruments or exclusion restrictions, the parametric
hypothesis add their interference which may be sometimes hard to
detect.  

However, even if these models are not popular in research any more,
they still serve as excellent teaching tools.  Heckman-type selection
models easily allow us to experiment with selection bias,
misspecification, exclusion restrictions etc.  These models are easy
to visualise and understand.


\subsection{Heckman's solution}

Look at the following (unobserved) structural process:
\begin{align}
  y_i^{S*} &= \beta^S x_i^S + \varepsilon_i^S
  \label{eq:probit*}
  \\
  y_i^{O*} &= \beta^O x_i^O + \varepsilon_i^O,
\end{align}
where $y_i^{S*}$ is the latent value of the selection process and
$y_i^{O*}$ is the latent outcome.  $\vec{x}_i^S$ and $\vec{x}_i^O$ are
explanatory variables for the selection and outcome processes.  Those
may or may not be equal.  We observe
\begin{align}
  y_i^S 
  &= 
  \begin{cases}
    0 & \quad \text{if } y_i^{S*} < 0
    \label{eq:probit}
    \\
    1 & \quad \text{otherwise}
  \end{cases}
  \\
  y_i^O
  &= 
  \begin{cases}
    0 & \quad \text{if} y_i^{S} = 0\\
    y_i^{O*} & \quad \text{otherwise},
  \end{cases}
\end{align}
i.e. we observe the outcome only if the latent selection variable is
positive.  The error term is assumed to follow a bi-variate normal
distribution: 
\begin{equation}
  \begin{pmatrix}
    \varepsilon^S\\
    \varepsilon^O
  \end{pmatrix}
  \sim
  N\left(
    \begin{pmatrix}
      0\\
      0
    \end{pmatrix},
    \begin{pmatrix}
      1 & \varrho\\
      \varrho & \sigma^2
    \end{pmatrix}
  \right).
\end{equation}
The observed dependence between $y^O$ and $x^O$ can now be written as
\begin{equation}
  \Exp [y^O|\vec{x}_i^O, y_i^S = 1] =
  \beta^O \vec{x}_i^O 
  +
  \Exp [ \varepsilon^O|\varepsilon^S \ge -\beta^S \vec{x}_i^S ].
\end{equation}
Estimating the equation above by OLS gives in general biased results,
as $\Exp [ \varepsilon^O|\varepsilon^S \ge -\beta^S \vec{x}_i^S ] \not
= 0$, unless $\varrho = 0$.  However, we may employ the follwing
simple strategy: we can find the expectations $\Exp [
\varepsilon^O|\varepsilon^S \ge -\beta^S \vec{x}_i^S ]$ by estimating
the selection equations \eqref{eq:probit*} and \eqref{eq:probit} by
probit, and thereafter insert these expectations into the OLS equation
as additional covariates \citep[see][for details]{greene2002}:
\begin{equation}
  y_i^O
  =
  \beta^O \vec{x}_i^O 
  + 
  \Exp [ \varepsilon^O|\varepsilon^S \ge
  -\beta^S \vec{x}_i^S ]
  +
  \eta_i
  \equiv
  \beta^O \vec{x}_i^O 
  + 
  \beta^\lambda \lambda(-\beta^S \vec{x}_i^S)
  +
  \eta_i
\end{equation}
where $\lambda(\cdot)$ is commonly referred to as inverse Mill's
ratio, and $\beta^\lambda = \varrho\sigma$.  Essentially, we describe
the selection problem as an omitted variable problem, with
$\lambda(\cdot)$ the omitted variable.

This is the original idea by \citet{heckman1976}.  As the model is
fully parametric, the more efficient ML estiation is straightforward
(see \citet{amemiya1985} for details).  The original article suggests
using the two-step solution for exploratory work and as the initial
value for maximum likelihood estimation -- the two-step solution costs
\$15 while the maximum-likelihood solution costs
\$700. %footnote 18, page 490
Nowadays, the costs are no issue any more.  However, two-step solution
allows certain generalisations more easily than ML.

The model allows a number of straightforward generalisations.  E.g. we
may assume that we have two outcome variables, one of which is
observable, depending on the selection process.  These models (a
version of Roy (....) model) are relevant for treatment effect
analysis.

This model, and it's derivations, were widely used in 1970s and 1980s.
Later, it has fallen into disfavour due to the reliance on the
parametric assumptions.  These are almoust never justified in social
sciences.  The model is well identified if the exclusion restriction
is fulfilled -- $\vec{x}^S$ includes a component, not present in
$\vec{x}^O$.  This means essentially that we have a valid instrument.
If this is not the case, the identification is related to the
non-linearity of the inverse Mill's ratio $\lambda(\cdot)$.  In order
to identify $\beta^\lambda$ and $\beta^O$, $\lambda(\cdot)$ must
differ from a linear combination of $\vec{x}^O$ components
\citep{leung+yu1996}.  The practical conclusion here is that standard
errors of the estimates depend on the variation in the latent
selection equation \eqref{eq:probit*}.  More variation gives smaller
standard errors\footnote{This is related to the identification on the
  margin....}.  During the recent decades, various semiparametric
estimation techinques have been increasingly used instead of the
Heckman model.




\input{selection_implement}

\input{selection_usage}

\input{selection_conclusion}

%% Note: If there is markup in \(sub)section, then it has to be escape as above.

\bibliography{selection}
%\bibliography{/home/suapm095/Documents/Literatur/literatur}

\end{document}