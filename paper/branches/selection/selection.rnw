% -*- mode: TeX-PDF-mode -*-
\documentclass[article]{jss}
\usepackage{amsmath,amssymb}
\usepackage{bbm}
%% need no \usepackage{Sweave.sty}

% ------- a few definitions ---------
\newcommand{\dnorm}{\phi}
\newcommand{\idiv}{\mathtt{//}}% integer division
\newcommand{\Natural}{\mathbb{N}}% Natural numbers.  needs ams-fonts
\newcommand{\pderiv}[2][]{\frac{\partial #1}{\partial #2}}
\newcommand{\Real}{\mathbb{R}}% Real numbers.  needs ams-fonts
\newcommand{\pnorm}{\Phi}
\newcommand{\var}{\mathrm{Var}\,}
\newcommand{\corr}{\mathrm{Corr}}
\DeclareMathOperator*{\Exp}{\mathbbm{E}}% expectation
%\newcommand{\Exp}[1][]{\mathrm{E}_{#1}}
\newcommand{\indic}{\mathbbm{1}}% indicator function
\newcommand{\laplace}{\mathcal{L}}% Laplace transform
\newcommand{\lik}{\mathcal{L}}% likelihood
\newcommand{\loglik}{\ell}% log likelihood
\newcommand*{\mat}[1]{\mathsf{#1}}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}
\newcommand{\dif}{\mathrm{d}} % diferentsiaalimärk
\newcommand{\me}{\mathrm{e}} % Konstant e=2,71828
% -----------------------

\author{Ott Toomet\\Tartu University
  \And
  Arne Henningsen\\University of Kiel}
\title{Sample Selection Models in \proglang{R}:\\
  Package \pkg{micEcon}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Arne Henningsen, Ott Toomet} %% comma-separated
\Plaintitle{Sample Selection Models in R: Package micEcon} %% without formatting
\Shorttitle{Sample Selection Models in R} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ 
  % 
  This paper describes the implementation of Heckman-type sample
  selection models in \proglang{R}.  We discuss the sample selection
  problem as well as the Heckman solution and argue that although modern
  econometrics has superior non- and semiparametric estimation methods
  in its toolbox, the Heckman models still serve as excellent
  teaching tools.  We describe the implementation of these models in
  package \pkg{micEcon} and illustrate the use of the package by
  showing Heckman selection models' strong and weak sides on several
  Monte Carlo and real data examples.
  %
}

\Keywords{sample-selection models, Heckman selection models, econometrics, \proglang{R}}
\Plainkeywords{sample-selection models, Heckman selection models, econometrics, R}

%% publication information
%% NOTE: This needs to filled out ONLY IF THE PAPER WAS ACCEPTED.
%% If it was not (yet) accepted, leave them commented.
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Ott Toomet\\
  Department of Economics\\
  Tartu University\\
  Narva 4-A123\\
  Tartu 51009, Estonia\\
  Telephone: +372 737 6348\\
  E-mail: \email{otoomet@ut.ee}\\
  URL: \url{http://www.obs.ee/~siim/}\\
  \\
   Arne Henningsen\\
   Department of Agricultural Economics\\
   University of Kiel\\
   24098 Kiel, Germany\\
   Telephone: +49 431 880 4445\\
   E-mail: \email{ahenningsen@agric-econ.uni-kiel.de}\\
   URL: \url{http://www.uni-kiel.de/agrarpol/ahenningsen/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Sample selection}


\subsection{The problem}

Social scientists are often interested in causal effects --- what is
the impact of a new drug, a certain type of school or being born as a
twin.  Many of these cases are not under researcher's control.  In
most cases, the subjects can decide themselves, whether they take the
drug or which school they attend.  They cannot control whether they
are twins, but neither can the researcher --- the twins may tend to be
born in different type of families than singles.  All these cases
are similar from the statistical point of view.  Whatever is the
sampling mechanism, from an initial ``random'' sample we extract a
sample of interest, which is not ``random'' any more
\citep[see][p.~1937, for a discussion]{heckman+macurdy1986}.

This problem --- people who are ``treated'' are different than the rest
of the population --- is usually referred to as the \emph{sample
  selection} or \emph{self-selection} problem.  We cannot estimate the
causal effect, unless we solve the selection problem.  Otherwise, we
never know which part of the observable outcome is related to the
causal relationship and which part is due to the fact that different
people were selected to the treatment and control groups.


\subsection{Possible solutions}

Solving the sample selection problems requires additional information.
The information can be in many possible forms which may or may not be
feasible or useful for any particular case.  Here we list a few
popular choices:

\begin{itemize}
\item Random experiment, the situation, where the participants
  \emph{do not} have control over their status while the researcher
  has.  Randomisation is in many cases the best of the possible
  methods as it is easy to analyse and understand.  However, this
  method is seldom feasible for practical and ethical reasons.  The
  experimental environment may add additional interference which
  complicate the analysis.
\item Instruments (exclusion restrictions) are in many ways similar to
  randomisation.  These are variables which researcher can observe,
  and which determine the treatment status but not the outcome.
  Unfortunately, these are in most cases contradicting requirements in
  the real life, and only seldom we have instruments of reasonable
  quality.  
\item Information about the functional form of the selection and
  outcome processes, such as the distribution of the disturbance
  terms.  The original Heckman's solution belong to this group.
  However, the functional form assuptions are usually hard to justify.
\end{itemize}

During the recent decades, either randomisation or the
pseudo-randomisation (natural experiments) has become the state-of-the
art while estimating the causal effects.  The methods, relying on the
distributional assumptions are becoming less widely used.  The reason
is obvious --- the parametric assumptions can only seldom be
justified, and we do not want our results to rely on dubious
assumptions.  Even if we have valid instruments or exclusion
restrictions, the parametric hypothesis add their interference which
may be hard to disentangle from the estimates.

However, even if these models are less popular in research nowadays,
they still serve as excellent teaching tools.  Heckman-type selection
models easily allow us to experiment with selection bias,
misspecification, exclusion restrictions etc.  These models are easy
to implement, to visualise and to understand.


\subsection{Heckman's solution}

Look at the following (unobserved) structural model:
\begin{align}
  y_i^{S*} &= {\vec{\beta}^S}' \vec{x}_i^S + \varepsilon_i^S
  \label{eq:probit*}
  \\
  y_i^{O*} &= {\vec{\beta}^O}' \vec{x}_i^O + \varepsilon_i^O,
\end{align}
where $y_i^{S*}$ is the realisation of the the latent value of the
selection ``tendency'' for the individual $i$, and $y_i^{O*}$ is the
latent outcome.  $\vec{x}_i^S$ and $\vec{x}_i^O$ are explanatory
variables for the selection and outcome equations.  $\vec{x}^S$ and
$\vec{x}^O$ may or may not be equal.  We observe
\begin{align}
  y_i^S 
  &= 
  \begin{cases}
    0 & \quad \text{if } y_i^{S*} < 0
    \label{eq:probit}
    \\
    1 & \quad \text{otherwise}
  \end{cases}
  \\
  y_i^O
  &= 
  \begin{cases}
    0 & \quad \text{if } y_i^{S} = 0\\
    y_i^{O*} & \quad \text{otherwise},
  \end{cases}
\end{align}
i.e. we observe the outcome only if the latent selection variable
$y^{S*}$ is positive.  The observed dependence between $y^O$ and $x^O$
can now be written as
\begin{equation}
  \E [y^O|\vec{x}^O = \vec{x}_i^O, \vec{x}^S = \vec{x}_i^S, y_i^S = 1] =
  {\vec{\beta}^O}' \vec{x}_i^O 
  +
  \E [ \varepsilon^O|\varepsilon^S \ge -{\vec{\beta}^S}' \vec{x}_i^S ].
\end{equation}
Estimating the model above by OLS gives in general biased results, as
$\E [ \varepsilon^O|\varepsilon^S \ge -{\vec{\beta}^S}' \vec{x}_i^S ]
\not = 0$, unless $\varrho = 0$.

However, assuming the error terms follow a bivariate normal
distribution:
\begin{equation}
  \begin{pmatrix}
    \varepsilon^S\\
    \varepsilon^O
  \end{pmatrix}
  \sim
  N\left(
    \begin{pmatrix}
      0\\
      0
    \end{pmatrix},
    \begin{pmatrix}
      1 & \varrho\\
      \varrho & \sigma^2
    \end{pmatrix}
  \right),
\end{equation}
we may employ the following simple strategy: we can find the
expectations $\E [ \varepsilon^O|\varepsilon^S \ge -{\vec{\beta}^S}'
\vec{x}_i^S ]$ by estimating the selection equations
\eqref{eq:probit*} and \eqref{eq:probit} by probit, and thereafter
insert these expectations into the OLS equation below, as additional
covariates (see \citet{greene2002} for details):
\begin{equation}
  y_i^O
  =
  {\vec{\beta}^O}' \vec{x}_i^O 
  + 
  \E [ \varepsilon^O|\varepsilon^S \ge
  -{\vec{\beta}^S}' \vec{x}_i^S ]
  +
  \eta_i
  \equiv
  {\vec{\beta}^O}' \vec{x}_i^O 
  + 
  \beta^\lambda \lambda(-{\vec{\beta}^S}' \vec{x}_i^S)
  +
  \eta_i
  \label{eq:tobit2_2step}
\end{equation}
where $\lambda(\cdot) = \phi(\cdot)/\Phi(\cdot)$ is commonly referred
to as inverse Mill's ratio, $\beta^\lambda = \varrho\sigma$, and
$\phi(\cdot)$ and $\Phi(\cdot)$ are standard normal density and
cumulative distribution functions.  Essentially, we describe the
selection problem as an omitted variable problem, with
$\lambda(\cdot)$ the omitted variable.

This is the original idea by \citet{heckman1976}.  As the model is
fully parametric, the more efficient ML estimatior is straightforward
to construct \citep[see][for details]{amemiya1985}.  The original
article suggests using the two-step solution for exploratory work and
as initial values for maximum likelihood estimation, since in those
days the cost of the two-step solution was \$15 while that of the
maximum-likelihood solution was \$700
\citep[p.~490]{heckman1976}. %footnote 18, page 490
Nowadays, the costs are no issue any more, however, the two-step solution
allows certain generalisations more easily than ML, and is more robust
in certain circumstances (see section~\ref{sec:additional} below).

The model allows a number of straightforward generalisations.  For
instance, we may assume that we have two outcome variables, only one
of which is observable, depending on the selection process
(``switching regression'' or ``tobit-5'' model, further details of
this classification are available in \citealp{amemiya1985}).  This
model may be relevant for treatment effect analysis.  Other types of
generalisations include less restrictive parametric forms for the
conditional mean of the residual term $\E [
\varepsilon^O|\varepsilon^S \ge -{\vec{\beta}^S}' \vec{x}_i^S ]$ (see
\citealp[p.~1938]{heckman+macurdy1986}, for references and
\citealp{klaauw+koning2003} for an application).

This model and its derivations were widely used in 1970s and 1980s.
Later, it has fallen into disfavour due to the reliance on the
parametric assumptions.  These are almost never justified in social
sciences.  The model is well identified if the exclusion restriction
is fulfilled, i.e.\ if $\vec{x}^S$ includes a component with a
substantial explanatory power, which is not present in $\vec{x}^O$.
This means essentially that we have a valid instrument.  If this is
not the case, the identification is related to the non-linearity of
the inverse Mill's ratio $\lambda(\cdot)$, exact form of which stems
from the distributional assumptions.  During the recent decades,
various semiparametric estimation techniques have been increasingly
used instead of the Heckman model (see \citealp{powell1994} and
\citealp{pagan+ullah1999} for a review).



\section[Implementation]{Implementation in \pkg{micEcon}}

The main frontend for the estimation of selection models in
\pkg{micEcon} is the command \code{selection}.  It needs a formula for
the selection equation (argument \code{selection}), and one (or a list
of two for switching regression models) for the outcome equation
(\code{outcome}).  One can choose the method (\code{method}) to be
either ``\code{ml}'' for the ML estimation, or ``\code{2step}'' for
the two-step method.  If the user does not provide initial values
(\code{start}) for the ML estimation (which is usually the case),
\code{selection} calculates consistent initial values by the two-step
method. These initial values are used in the corresponding ML
estimation functions \code{tobit2fit} or \code{tobit5fit}.  Note that
the log-likelihood functions of the selection models are in general
not globally concave, and hence, one should use good initial values
(see the example in Secion~\ref{sec:additional}).  The two-step
estimation is done by the function \code{heckit2} or \code{heckit5},
depending on the model.

The likelihood maximisation is done by the Newton-Raphson maximisation method
(implemented as \code{maxNR} in \pkg{micEcon}), where an analytic score
vector and an analytic Hessian matrix are used.  This results in reasonably fast
computation times even in the case of tens of thousands observations.
A well-defined model should converge in less than 15 iterations; in
the case of weak identification, this number may be considerably
larger.  Convergence issues may appear at the boundary of the
parameter space (e.g.\ if $\varrho \to 1$) or if the algorithm does not
find the global maximum.



\section[Usage]{Using the \code{selection} function}

This section provides selected illustrative Monte-Carlo experiments
which illustrate both the strong and weak sides of the method, and the
typical usage of \code{selection}.


\subsection{Tobit-2 models}
\label{sec:tobit2}

First, we estimate a correctly specified tobit-2 model with exclusion
restriction:
<<code:t2generate>>=
  set.seed(0)
  library(micEcon)
  library(mvtnorm)
  eps <- rmvnorm(500, c(0,0), matrix(c(1,-0.7,-0.7,1), 2, 2))
  xs <- runif(500)
  ys <- xs + eps[,1] > 0
  xo <- runif(500)
  yo <- (xo + eps[,2])*(ys > 0)
@
% -- please keep these comments.  These are necessary for auctex
We use \pkg{mvtnorm} in order to create bivariate normal disturbances
with correlation $-0.7$.  Next, we generate a uniformly distributed
explanatory variable for the selection equation, \code{xs} and the
selection outcome by a probit data generating process.  Note that the
vectors of explanatory variables for the selection (\code{xs}) and
outcome equation (\code{xo}) are independent and hence the exclusion
restriction is fulfilled.  This can be seen from the fact that the
estimates are reasonably precise:
<<code:t2summary>>=
  summary(selection(ys~xs, yo ~xo))
@ 
%
One can see, that all the true values are within the 95\% confidence
intervals of the corresponding estimates.

Now we repeat the same exercise, but without exclusion restriction,
generating \code{yo} using \code{xs} instead of \code{xo}.
<<>>=
yo <- (xs + eps[,2])*(ys > 0)
print(summary(selection(ys ~ xs, yo ~ xs)))
@
% 
That the standard errors are substantially larger in this case.  The
exclusion restriction --- information about the selection process ---
has a certain identifying power which we have lost now.  We are solely
relying on the functional form identification.  

In order to identify $\beta^\lambda$ and ${\vec{\beta}^O}'$ without
exclusion restriction, $\lambda(\cdot)$ must differ from a linear
combination of $\vec{x}^O$ components \citep{leung+yu1996}.  The
degree of non-linearity in $\lambda(\cdot)$ depends on the variability
in ${\vec{\beta}^S}' \vec{x}^S$ as $\lambda(\cdot)$ is a smooth convex
function (see Figure~\ref{fig:lambda()} below).  Hence the standard
errors of the estimates depend on the variation in the latent
selection equation \eqref{eq:probit*}, even without the exclusion
restriction fulfilled.  More variation gives smaller standard
errors\footnote{This is related to the identification at infinity
  \citep{chamberlain1986}}.  We demonstrate it below: Change the
support of \code{xs} from $[0,1]$ to $[-5,5]$:
<<>>=
  xs <- runif(500, -5, 5)
  ys <- xs + eps[,1] > 0
yoX <- xs + eps[,2]
  yo <- yoX*(ys > 0)
@ 
%
where we retain the unobserved outcome variable \code{yoX} for the
figure below.
<<>>=
  print(summary(m <- selection(ys ~ xs, yo ~ xs)))
@
%
Now all the parameters are precisely estimated, with higher precision
than in the case of exclusion restriction.  The reason is simple: the
probability that $Y^{O*}$ is observable, given $x_i^S$, is $\Pr(1
\cdot x_i^S + \varepsilon^O > 0) = 1 - \Phi(-x_i^S)$ as $\varepsilon^O
\sim N(0,1)$.  As one can see from Figure~\ref{fig:lambda()},
selection is not an issue if $x^S \gtrsim 2$.  In that case $\E
[\varepsilon^O|y^S = 1] \approx 0$ and hence the model can be
precisely estimated for relatively large values of $x^S$.  Due to the
linearity assumption, the same parameters are valid everywhere.  In
the former example, the variation in $x^S$ was low and hence one could
not disentangle the selection bias from the true causal dependence.
Technically, there was a multicollinearity problem between $x$ and
$\lambda(\cdot)$.

\begin{figure}[tp]
  \centering
<<fig=TRUE,height=4,echo=FALSE>>=
par(mar=c(3,3,0,0) + 0.1,
    mgp=c(2,1,0))
pch <- c(1, 16)
plot(xs, yoX, pch=pch[1 + ys], cex=0.5, lwd=0.3)
abline(a=0, b=1, lty=1)
# True dependence
abline(a=coef(m)[3], b=coef(m)[4], lty=2)
# Heckman's model
cf <- coef(lm(yo ~ xo, subset=ys==1))
abline(a=cf[1], b=cf[2], lty=3)
# OLS
@ 
%
\caption{The observable (filled) and unobservable (empty circles),
  given \code{xs}.  The solid line denote the true dependence, dashed
  (almoust overlapping the solid) is the MLE estimate above, and
  the dotted line is the corresponding OLS estimate}
  \label{fig:lambda()}
\end{figure}



\subsection{Switching regression models}
\label{sec:tobit5}

Now let us focus on the tobit-5 examples.  This type of problems arise
in a wide variety of contexts, e.g.\ in treatment effect or schooling
choice analysis.  The problem can be written as a system of three
latent simultaneous equations:
\begin{align}
  y_i^{S*} &= {\vec{\beta}^S}' \vec{x}_i^S + \varepsilon_i^S
  \\
  y_i^{O1*} &= {\vec{\beta}^{O1}}' \vec{x}_i^O + \varepsilon_i^{O1}
  \\
  y_i^{O2*} &= {\vec{\beta}^{O2}}' \vec{x}_i^O + \varepsilon_i^{O2},
\end{align}
where $y^{S*}$ is the selection ``tendency'' as in the case of tobit-2
models, and $y^{O1*}$ and $y^{O2*}$ are the latent outcomes, only one
of which is observable, depending on the sign of $y^{S*}$.  Hence we
observe
\begin{align}
  y_i^S 
  &= 
  \begin{cases}
    0 & \quad \text{if } y_i^{S*} < 0
    \\
    1 & \quad \text{otherwise}
  \end{cases}
  \\
  y_i^O
  &= 
  \begin{cases}
    y_i^{O1*} & \quad \text{if } y_i^{S} = 0\\
    y_i^{O2*} & \quad \text{otherwise}.
  \end{cases}
\end{align}
We assume the disturbance terms have a 3-dimensional normal
distribution:
\begin{equation}
  \begin{pmatrix}
    \varepsilon^S\\
    \varepsilon^{O1}\\
    \varepsilon^{O2}
  \end{pmatrix}
  \sim
  N\left(
    \begin{pmatrix}
      0\\
      0\\
      0
    \end{pmatrix},
    \begin{pmatrix}
      1                 & \varrho_1\sigma_1 & \varrho_2\sigma_2 \\
      \varrho_1\sigma_1 & \sigma_1^2        & \varrho_{12}\sigma_1\sigma_2\\
      \varrho_2\sigma_2 & \varrho_{12}\sigma_1\sigma_2 & \sigma_2^2
    \end{pmatrix}
  \right).
\end{equation}
It is straightforward to construct analogous two-step estimators
as~\eqref{eq:tobit2_2step} and the corresponding ML estimator.  Note,
that $\varrho_{12}$ plays no role in this model, the observable
distributions are determined by the correlations $\varrho_1$ and
$\varrho_2$ between the disturbances of the selection equation
($\varepsilon^S$) and the corresponding outcome equation
($\varepsilon^{O1}$ and $\varepsilon^{O2}$).

We create the following simple switching regression problem:
<<>>=
  set.seed(0)
  vc <- diag(3)
  vc[lower.tri(vc)] <- c(0.9, 0.5, 0.1)
  vc[upper.tri(vc)] <- vc[lower.tri(vc)]
  eps <- rmvnorm(500, rep(0, 3), vc)
  xs <- runif(500)
  ys <- xs + eps[,1] > 0
  xo1 <- runif(500)
  yo1 <- xo1 + eps[,2]
  xo2 <- runif(500)
  yo2 <- xo2 + eps[,3]
@ 
%
We generate a 3-dimensional disturbance vector by \code{rmvnorm}.  We
set the correlation between $\varepsilon^S$ and $\varepsilon^{O1}$ to
equal 0.9 and between $\varepsilon^S$ and $\varepsilon^{O2}$ to 0.5.
The third correlation, 0.1, takes care for the positive definiteness
of the covariance matrix and does not affect the results.  Further, we
create three independent explanatory variables (\code{xs}, \code{xo1}
and \code{xo2}, uniformly distributed on $[0,1]$, and hence the
exclusion restriction is fulfilled).  Note that we do not have to
remove the unobserved values from outcome vectors, those are simply
ignored.  The results look as follows:
<<>>=
  summary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2)))
@ 
We can see, that the parameters are fairly well estimated.  All the
estimates are close to the true values.

Next, take an example of functional form misspecification.  We create
the disturbances as 3-variate $\chi_1^2$ random variables (we subtract
1 in order to get the mean zero disturbances), and generate \code{xs}
to be in the interval $[-1,0]$ in order to get asymmetric distribution
over observed choices:
<<>>=  
set.seed(5)
eps <- rmvnorm(1000, rep(0, 3), vc)
eps <- eps^2 - 1
xs <- runif(1000, -1, 0)
ys <- xs + eps[,1] > 0
xo1 <- runif(1000)
yo1 <- xo1 + eps[,2]
xo2 <- runif(1000)
yo2 <- xo2 + eps[,3]

summary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2), iterlim=20))
@
%
Although we still have an exclusion restriction, now serious problems
appear --- most intercepts are statistically significant, whereas the
true values are zero.  The model has serious convergence problems,
often it does not converge at all (this is why we increased the
\code{iterlim} here while using \code{set.seed(5)}).

As the last tobit example, we repeat the previous excercise without
the exclusion restriction, and a slightly larger variance of \code{xs}:
<<code:t5chi_woER>>=
set.seed(6)
xs <- runif(1000, -1, 1)
  ys <- xs + eps[,1] > 0
  yo1 <- xs + eps[,2]
  yo2 <- xs + eps[,3]
summary(tmp <- selection(ys~xs, list(yo1 ~ xs, yo2 ~ xs), iterlim=20))
@
%
In most cases, this model does not converge.  However, if it does
(like in this case, where we use \code{set.seed(6)}),
the results may be seriously biased.
Note that first outcome parameters have low standard errors, but a
serious bias.  We present the graph of the correct control function
(based on the $\chi^2$ distribution),
and one where we assume the normal distribution of the disturbance
terms in Figure~\ref{fig:cfunctions}.  We use the estimated parameters
for constructing the latter, however, we scale the normal control
functions (inverse Mill's ratios) to a roughly similar scale, as the
correct ones.
\begin{figure}[htbp]
  \centering
<<fig=TRUE,height=4,echo=FALSE>>=
   EUlower <- function(alpha) {
      alpha[alpha >= 1] <- NA
      alpha <- sqrt(-alpha + 1)
      EUalpha <- ss^2 - 2*ss*alpha*dnorm(alpha/ss)/(1 - 2*pnorm(-alpha/ss))
      s1s^2/ss^2*EUalpha + s1^2 - s1s^2/ss^2 - 1
   }
   EUupper <- function(alpha) {
      alpha[alpha >= 1] <- 1
      alpha <- sqrt(-alpha + 1)
      EUalpha <- (ss*alpha*dnorm(alpha/ss) + ss^2*(1 - pnorm(alpha/ss)))/(1 - pnorm(alpha/ss))
      s2s^2/ss^2*EUalpha + s2^2 - s2s^2/ss^2 - 1
   }
   Nlower <- function(alpha) {
      alpha <- -alpha
      -Ns1s*dnorm(alpha)/pnorm(alpha)
   }
   Nupper <- function(alpha) {
      alpha <- -alpha
      Ns2s*dnorm(-alpha)/pnorm(-alpha)
   }
   ss <- sqrt(vc[1,1])
   s1 <- sqrt(vc[2,2])
   s2 <- sqrt(vc[3,3])
   s1s <- vc[1,2]
   s2s <- vc[1,3]
   Ns1s <- coef(tmp)["sigma1"]*coef(tmp)["rho1"]
   Ns2s <- coef(tmp)["sigma2"]*coef(tmp)["rho2"]
hatb1O <- coef(tmp)[c("XO1(Intercept)", "XO1xs")]
hatb2O <- coef(tmp)[c("XO2(Intercept)", "XO2xs")]
   es <- eps[,1]
   e1 <- eps[,2]
   e2 <- eps[,3]
   ex <- seq(-5, 5, length=200)
ey <- cbind(EUlower(ex), EUupper(ex),
            -31*Nupper(cbind(1, ex)%*%hatb1O), 5.5*Nlower(cbind(1,ex)%*%hatb2O)
###         -s1s*dnorm(-ex)/pnorm(-ex), s2s*dnorm(ex)/pnorm(ex)
            )
   mcy <- matrix(0, length(ex), 2)
   for(i in seq(length=nrow(mcy))) {
      mcy[i,1] <- mean(e1[es < -ex[i]])
      mcy[i,2] <- mean(e2[es > -ex[i]])
   }
par(cex=0.8, mar=c(3,3,0,0) + 0.1, mgp=c(2,1,0))
   matplot(ex, ey, type="l", lty=c(1,1,2,2,3,3), col=1,
           xlab=expression(x^S), ylab="",
           ylim=c(-1.5,2.5))
abline(v=-1,lty=3)
abline(v=1, lty=3)
#   matpoints(ex, mcy, pch=c(1,2), cex=0.5, col=1)
   axis(4)
text(-3.5, 0.8, 
     expression(paste("correct  ")*E*group("[", Epsilon^{O1}*group("|", Epsilon^S > -x^S, ""), "]")))
text(-3.6, -0.4,
     expression(paste("correct  ")*E*group("[", Epsilon^{O1}*group("|", Epsilon^S < -x^S, ""), "]")))
text(-0.7, 1.5,
     expression(E*group("[", Epsilon^{O1}*group("|", 
         Epsilon^S > -bold(beta)^S*minute*bold(x)^S, ""), "]")*
         paste("  assumed normal")))
text(2.2, -1.4,
     expression(E*group("[", Epsilon^{O1}*group("|", 
         Epsilon^S < -bold(beta)^S*minute*bold(x)^S, ""), "]")*
         paste("  assumed normal")))
@ 
  \medskip \\
  \small Note:
  Dotted vertical lines denote the support of $x^S$ in the MC experiment.
  \caption{Control functions: correct ($\chi^2$ distributed)
    and assumed (normally distributed)}
  \label{fig:cfunctions}
\end{figure}
One can see, that the functions differ substantially in the relevant
range of $x^S \in [-1, 1]$.  In particular, the true upper mean rises
substantially faster close to $x^S = 1$ than the normal approximation.
The correct lower mean is decreasing slightly faster compared to the
approximation.

It is instructive to estimate the same model as a two independent OLS
equations:
<<>>=
  summary(lm(yo1~xs, subset=ys==0))
  summary(lm(yo2~xs, subset=ys==1))
@ 
%
One can see that the OLS estimates are very close to the ML ones.
This is related to the fact that none of the $\varrho$-s is
statistically significant.


\section{Testing Reliability}

In this section we test the reliability of the results from \code{selection}
applying the two-step and the maximum likelihood estimation method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Greene( 2002 ): example 22.8, page 786}

The first test is example 22.8 from \citet[p.~786]{greene2002}.
The data set used in this example is included in \pkg{micEcon};
it is called \code{Mroz87}.
This data set was used by \citet{mroz1987} for analysing female labour supply.
In this example, labour force participation (described by dummy \code{lfp})
is modelled by a quadratic polynomial in age (\code{age}),
family income (\code{faminc}, in 1975 dollars),
presence of children (\code{kids}),
and education in years (\code{educ}).
The wage equation includes
a quadratic polynomial in quadratic polynomial in experience (\code{exper}),
education in years (\code{educ}),
and residence in a big city (\code{city}).
First, we have to create a dummy variable for presence of children.
<<greene22.8start>>=
data( Mroz87 )
Mroz87$kids <- ( Mroz87$kids5 + Mroz87$kids618 > 0 )
@
Now, we estimate the model by the two-step method
<<greene22.8TwoStep>>=
summary( tmp <- selection( lfp ~ age + I( age^2 ) + faminc + kids + educ,
   wage ~ exper + I( exper^2 ) + educ + city,
   data = Mroz87, method = "2step" ) )
tmp$coefficients[ "invMillsRatio" ]
tmp$vcov[ "invMillsRatio", "invMillsRatio" ]^0.5
tmp$rho
tmp$sigma
@
Most results are identical to the values reported by
\citet[p.~786]{greene2002}.
Only the coefficient of the inverse Mill's ratio ($\rho \sigma$),
its standard error, and $\rho$ deviate from the published results,
but all differences are less than one percent.%
\footnote{
Note that the standard error of the inverse Mill's ratio
($\rho \sigma$) is wrong in \citet[p.~786]{greene2002}
(see errata on
http://pages.stern.nyu.edu/~wgreene/Text/Errata/ERRATA5.htm).
}

Finally, we repeat the analysis with the maximum likelihood estimation method:
<<greene22.8ML>>=
summary( selection( lfp ~ age + I( age^2 ) + faminc + kids + educ,
   wage ~ exper + I( exper^2 ) + educ + city, data = Mroz87 ) )
@
While the estimated coefficients are almost identical to the values
published in the errata of \citet{greene2002}
(\url{http://pages.stern.nyu.edu/~wgreene/Text/Errata/ERRATA5.htm}),
the standard errors calculated by \code{selection} are considerably
smaller.
% WHY???? We have to check this!!!!!


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Additional notes}
\label{sec:additional}

The log-likelihood of the models above is not globally concave.  It means, that the model may not
converge, if the initial values are not chosen well enough.  This may easily happen which we illustrate
below, while we also give a taste of the real data.  Consider the following example:
<<init_2step>>=
data(Mroz87)
mr <- selection(lfp ~ kids5 + poly(age, 2) + educ + log(huswage) + mtr + fatheduc + city + unem,
log(hours) ~ kids5 + poly(age, 2) + educ + wage + mtr + city + poly(exper, 2),
data=Mroz87)
@ 
%
We use a dataset, \code{Mroz87}, included in \pkg{micEcon}.  This data
was used for analysing the female labour supply by \citet{mroz1987}.
We model the labour force participation (described by dummy
\code{lfp}) by presence of young children (\code{kids5}), quadratic
polynomial in age, education in years (\code{educ}), log husbands wage
(\code{huswage}), marginal tax rate (\code{mtr}), father education (in
years, \code{fatheduc}), residence in a big city (\code{city}) and the
local unemployment rate (\code{unem}).  The labour supply equation
includes most of the variables above (leaving \code{huswage},
\code{fathedu} and \code{unem} for exclusion restrictions).
Additional explanatory variables are woman's wage (\code{wage}) and
quadratic polynomial in experience (\code{exper}).

The model gives reasonable results:
<<init_0>>=
summary(mr)
@ 
%
One may see, that small children in the family, high salary of husband
and high marginal tax rate decrease the participation probability,
while more education and residence in a big city increase it.  The
labour supply equation seems a bit less convincing with more education
and income leading to less hours worked and small kids causing women
to work more.  The correlation of error terms \code{rho} shows a
nearly perfect negative correlation between the corresponding error
terms, which means we estimate essentially a tobit model (see
\citet{mroz1987} for more results and discussion).

However, the results are sensitive to the start values
(note that you have to supply a positive initial value for the
variance):
<<>>=
mr <- selection(lfp ~ kids5 + poly(age, 2) + educ + log(huswage) + mtr + fatheduc + city + unem,
log(hours) ~ kids5 + poly(age, 2) + educ + wage + mtr + city + poly(exper, 2),
data=Mroz87, start=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0))
summary(mr)
@ 
%
The maximisation routine gets stuck in a non-concave region and
cannot find the global maximum.  Here another optimisation algorithm
might be useful, however, switch of the algorithm is not yet
implemented.  In far the most cases, the 2-step method, used for
calculating the initial values, does a good job.

In general, one should prefer \code{method="ml"} instead of
\code{"2step"}.  However, ML may have problems at the boundary of the
parameter space.  Take the textbook tobit example: 
\begin{equation}
  \label{eq:tobit}
  y_i^* = \vec{\beta}' \vec{x}_i + \varepsilon_i;
  \quad
  y_i =
  \begin{cases}
    y_i^* \quad & \text{if } y_i^* > 0\\
    0           & \text{otherwise.}
  \end{cases}
\end{equation}
This model can be written as a tobit-2 model where the error term of
the selection and outcome equation are perfectly correlated.  In this
case the ML may not converge:
<<tobit_tobit2>>=
set.seed(0)
x <- runif(1000)
y <- x + rnorm(1000)
ys <- y > 0
summary(selection(ys~x, y~x))
@ 
%
The reason is, that $\varrho = 1$ lies at the boundary of the
parameter space.  However, the 2-step method still works, although
standard errors are large:
<<>>=
summary(selection(ys~x, y~x, method="2step"))
@ 
%



\section{Conclusions}

This paper describes the Heckman-type selection models and their
implementation in \pkg{micEcon} for programming language \proglang{R}.
We argue, that these models serve as useful teaching tools as they are
easy to implement and understand, although nowadays there exist
superior estimation methods for censored data.

We briefly describe the implementation and usage of tobit-2 and
tobit-5 (switching regression) models in \pkg{micEcon}, and
demonstrate the virtues and caveats of them using a number of
examples.  Our implementation works well for correctly specified
cases, in the case of severe misspecification or weak identification,
a more robust maximisation algorithm might be useful.  This is left
for the future work.



\bibliography{selection}

\end{document}
