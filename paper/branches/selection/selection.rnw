% -*- mode: TeX-PDF-mode -*-
\documentclass[article]{jss}
\usepackage{amsmath}
\usepackage{bbm}
%% need no \usepackage{Sweave.sty}

% ------- a few definitions ---------
\newcommand{\dnorm}{\phi}
\newcommand{\idiv}{\mathtt{//}}% integer division
\newcommand{\Natural}{\mathbb{N}}% Natural numbers.  needs ams-fonts
\newcommand{\pderiv}[2][]{\frac{\partial #1}{\partial #2}}
\newcommand{\Real}{\mathbb{R}}% Real numbers.  needs ams-fonts
\newcommand{\pnorm}{\Phi}
\newcommand{\var}{\mathrm{Var}\,}
\newcommand{\corr}{\mathrm{Corr}}
\DeclareMathOperator*{\Exp}{\mathbbm{E}}% expectation
%\newcommand{\Exp}[1][]{\mathrm{E}_{#1}}
\newcommand{\indic}{\mathbbm{1}}% indicator function
\newcommand{\laplace}{\mathcal{L}}% Laplace transform
\newcommand{\lik}{\mathcal{L}}% likelihood
\newcommand{\loglik}{\ell}% log likelihood
\newcommand*{\mat}[1]{\mathsf{#1}}
\renewcommand*{\vec}[1]{\boldsymbol{#1}}
\newcommand{\dif}{\mathrm{d}} % diferentsiaalimärk
\newcommand{\me}{\mathrm{e}} % Konstant e=2,71828
% -----------------------

\author{Ott Toomet\\Tartu University
  \And
  Arne Henningsen\\University of Kiel}
\title{Sample Selection Models in \proglang{R}:\\
  Package \pkg{micEcon}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Arne Henningsen, Ott Toomet} %% comma-separated
\Plaintitle{Sample Selection Models in R: Package micEcon} %% without formatting
\Shorttitle{Sample Selection Models in R} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ 
  % 
  This paper describes the implementation of Heckman-type sample
  selection models in \proglang{R}.  We discuss the sample selection
  problem as well as the Heckman solution and argue that although modern
  econometrics has superior non- and semiparametric estimation methods
  in its toolbox, the Heckman models still serve as excellent
  teaching tools.  We describe the implementation of these models in
  package \pkg{micEcon} and illustrate the use of the package by
  showing Heckman selection models' strong and weak sides on several
  Monte Carlo and real data examples.
  %
}

\Keywords{sample-selection models, Heckman selection models, econometrics, \proglang{R}}
\Plainkeywords{sample-selection models, Heckman selection models, econometrics, R}

%% publication information
%% NOTE: This needs to filled out ONLY IF THE PAPER WAS ACCEPTED.
%% If it was not (yet) accepted, leave them commented.
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Ott Toomet\\
  Department of Economics\\
  Tartu University\\
  Narva 4-A123\\
  Tartu 51009, Estonia\\
  Telephone: +372 737 6348\\
  E-mail: \email{otoomet@ut.ee}\\
  URL: \url{http://www.obs.ee/~siim/}\\
  \\
   Arne Henningsen\\
   Department of Agricultural Economics\\
   University of Kiel\\
   24098 Kiel, Germany\\
   Telephone: +49 431 880 4445\\
   E-mail: \email{ahenningsen@agric-econ.uni-kiel.de}\\
   URL: \url{http://www.uni-kiel.de/agrarpol/ahenningsen/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Sample selection}


\subsection{The problem}

Social scientists are often interested in causal effects --- what is
the impact of a new drug, a certain type of school or being born as a
twin.  Many of these cases are not under researcher's control.  In
most cases, the subjects can decide themselves, whether they take the
drug or which school they attend.  They cannot control whether they
are twins, but neither can the researcher --- the twins may tend to be
born in different types of families than singles.  All these cases
are similar from the statistical point of view.  Whatever is the
sampling mechanism, from an initial ``random'' sample we extract a
sample of interest, which is not ``random'' any more
\citep[see][p.~1937, for a discussion]{heckman+macurdy1986}.

This problem --- people who are ``treated'' are different than the rest
of the population --- is usually referred to as the \emph{sample
  selection} or \emph{self-selection} problem.  We cannot estimate the
causal effect, unless we solve the selection problem.  Otherwise, we
never know which part of the observable outcome is related to the
causal relationship and which part is due to the fact that different
people were selected to the treatment and control groups.


\subsection{Possible solutions}

Solving the sample selection problems requires additional information.
The information can be in many possible forms which may or may not be
feasible or useful for any particular case.  Here we list a few
popular choices:

\begin{itemize}
\item Random experiment, the situation, where the participants
  \emph{do not} have control over their status while the researcher
  has.  Randomisation is in many cases the best of the possible
  methods as it is easy to analyse and understand.  However, this
  method is seldom feasible for practical and ethical reasons.  The
  experimental environment may add additional interference which
  complicate the analysis.
\item Instruments (exclusion restrictions) are in many ways similar to
  randomisation.  These are variables which researcher can observe,
  and which determine the treatment status but not the outcome.
  Unfortunately, these are in most cases contradicting requirements in
  the real life, and only seldom we have instruments of reasonable
  quality.
\item Information about the functional form of the selection or
  outcome processes.  For instance, assumptions about the distribution
  of the error terms.  These assumptions may be easy to investigate 
\item Timing information.  In some cases, relative timing of the
  events may give us information, necessary for identification of the
  causal effect.
\end{itemize}

During the recent decades, either randomisation or the
pseudo-randomisation (natural experiments) has become the state-of-the
art while estimating the causal effects.  The methods, relying on the
distributional assumptions are becoming less widely used.  The reason
is obvious --- the parametric assumptions can only seldom be justified,
and we do not want our results to rely on dubious assumptions.  Even
if we have valid instruments or exclusion restrictions, the parametric
hypothesis add their interference which may be sometimes hard to
disentangle from the estimate.

However, even if these models are not popular in research any more,
they still serve as excellent teaching tools.  Heckman-type selection
models easily allow us to experiment with selection bias,
misspecification, exclusion restrictions etc.  These models are easy
to implement, to visualise and to understand.


\subsection{Heckman's solution}

Look at the following (unobserved) structural process:
\begin{align}
  y_i^{S*} &= \beta^S x_i^S + \varepsilon_i^S
  \label{eq:probit*}
  \\
  y_i^{O*} &= \beta^O x_i^O + \varepsilon_i^O,
\end{align}
where $y_i^{S*}$ is the latent value of the selection process and
$y_i^{O*}$ is the latent outcome.  $\vec{x}_i^S$ and $\vec{x}_i^O$ are
explanatory variables for the selection and outcome processes.  Those
may or may not be equal.  We observe
\begin{align}
  y_i^S 
  &= 
  \begin{cases}
    0 & \quad \text{if } y_i^{S*} < 0
    \label{eq:probit}
    \\
    1 & \quad \text{otherwise}
  \end{cases}
  \\
  y_i^O
  &= 
  \begin{cases}
    0 & \quad \text{if} y_i^{S} = 0\\
    y_i^{O*} & \quad \text{otherwise},
  \end{cases}
\end{align}
i.e.\ we observe the outcome only if the latent selection variable is
positive.  The error term is assumed to follow a bivariate normal
distribution: 
\begin{equation}
  \begin{pmatrix}
    \varepsilon^S\\
    \varepsilon^O
  \end{pmatrix}
  \sim
  N\left(
    \begin{pmatrix}
      0\\
      0
    \end{pmatrix},
    \begin{pmatrix}
      1 & \varrho\\
      \varrho & \sigma^2
    \end{pmatrix}
  \right).
\end{equation}
The observed dependence between $y^O$ and $x^O$ can now be written as
\begin{equation}
  \E [y^O|\vec{x}_i^O, y_i^S = 1] =
  \beta^O \vec{x}_i^O 
  +
  \E [ \varepsilon^O|\varepsilon^S \ge -\beta^S \vec{x}_i^S ].
\end{equation}
Estimating the equation above by OLS gives in general biased results,
as $\E [ \varepsilon^O|\varepsilon^S \ge -\beta^S \vec{x}_i^S ] \not
= 0$, unless $\varrho = 0$.  However, we may employ the following
simple strategy: we can find the expectations $\E [
\varepsilon^O|\varepsilon^S \ge -\beta^S \vec{x}_i^S ]$ by estimating
the selection equations \eqref{eq:probit*} and \eqref{eq:probit} by
probit, and thereafter insert these expectations into the OLS equation
as additional covariates \citep[see][for details]{greene2002}:
\begin{equation}
  y_i^O
  =
  \beta^O \vec{x}_i^O 
  + 
  \E [ \varepsilon^O|\varepsilon^S \ge
  -\beta^S \vec{x}_i^S ]
  +
  \eta_i
  \equiv
  \beta^O \vec{x}_i^O 
  + 
  \beta^\lambda \lambda(-\beta^S \vec{x}_i^S)
  +
  \eta_i
\end{equation}
where $\lambda(\cdot)$ is commonly referred to as inverse Mill's
ratio, and $\beta^\lambda = \varrho\sigma$.  Essentially, we describe
the selection problem as an omitted variable problem, with
$\lambda(\cdot)$ the omitted variable.

This is the original idea by \citet{heckman1976}.  As the model is
fully parametric, the more efficient ML estimation is straightforward
\citep[see][for details]{amemiya1985}.  The original article suggests
using the two-step solution for exploratory work and as initial
values for maximum likelihood estimation,
since in those days the two-step solution costs
\$15 while the maximum-likelihood solution costs
\$700 \citep[p.~490]{heckman1976}. %footnote 18, page 490
Nowadays, the costs are no issue any more.  However, two-step solution
allows certain generalisations more easily than ML.

The model allows a number of straightforward generalisations.
For instance, we
may assume that we have two outcome variables, one of which is
observable, depending on the selection process (``switching regression''
or ``tobit-5'' model, further details of this classification
are available in \citealp{amemiya1985}).
This model may be relevant for treatment effect
analysis.  Other types of generalisations include less restrictive
parametric forms for the conditional mean of the residual term $\E [
\varepsilon^O|\varepsilon^S \ge -\beta^S \vec{x}_i^S ]$
(see \citealp[p.~1938]{heckman+macurdy1986}, for references and
\citealp{klaauw+koning2003} for an application).

This model and its derivations were widely used in 1970s and 1980s.
Later, it has fallen into disfavour due to the reliance on the
parametric assumptions.  These are almost never justified in social
sciences.  The model is well identified if the exclusion restriction
is fulfilled, i.e.\ if $\vec{x}^S$ includes a component, that is not present in
$\vec{x}^O$.  This means essentially that we have a valid instrument.
If this is not the case, the identification is related to the
non-linearity of the inverse Mill's ratio $\lambda(\cdot)$.  In order
to identify $\beta^\lambda$ and $\beta^O$, $\lambda(\cdot)$ must
differ from a linear combination of $\vec{x}^O$ components
\citep{leung+yu1996}.  The practical conclusion here is that standard
errors of the estimates depend on the variation in the latent
selection equation \eqref{eq:probit*}.  More variation gives smaller
standard errors\footnote{This is related to the identification at
  infinity \citep{chamberlain1986}}.  During the recent decades,
various semiparametric estimation techniques have been increasingly
used instead of the Heckman model (see \citealp{powell1994} and
\citealp{pagan+ullah1999} for a review).



\section[Implementation]{Implementation in \pkg{micEcon}}

The main frontend for the estimation of selection models
in \pkg{micEcon} is the command \code{selection}.
It needs a formula for the selection equation (argument \code{selection}),
and one (or a list of two for tobit-5 models)
for the outcome equation (\code{outcome}).  One can choose the method
(\code{method}) to be either ``\code{ml}'' for the ML estimation, or
``\code{2step}'' for the two-step method.  If the user does not provide
initial values (\code{start}) for the ML estimation (which is usually
the case), \code{selection} calculates consistent initial
values by the two-step method. These initial
values are used in the corresponding ML estimation functions
\code{tobit2fit} or \code{tobit5fit}.  Note that the log-likelihood
functions of the selection models are in general not globally
concave, and hence, one should use as good initial values as possible.  
The two-step estimations are done either by function
\code{heckit2} or by function \code{heckit5}.

The likelihood maximisation is done by the Newton-Raphson maximisation method
(implemented as \code{maxNR} in \pkg{micEcon}), where an analytic score
vector and an analytic Hessian matrix are used.  This results in reasonably fast
computation times even in the case of tens of thousands observations.
A well-defined model should converge in less than 15 iterations; in
the case of weak identification, this number may be considerably
larger.  Convergence issues may appear at the boundary of the
parameter space (e.g.\ if $\varrho \to 1$) or if the algorithm does not
find the global maximum.



\section[Usage]{Using the \code{selection} function}

In this section we show a few examples on the typical usage of
\code{selection}.  


\subsection{Tobit-2 models}
\label{sec:tobit2}

First, estimate a correctly specified tobit-2 model with exclusion
restriction: 
<<code:t2generate>>=
  set.seed(0)
  library(micEcon)
  library(mvtnorm)
  eps <- rmvnorm(500, c(0,0), matrix(c(1,-0.7,-0.7,1), 2, 2))
  xs <- runif(500)
  ys <- xs + eps[,1] > 0
  xo <- runif(500)
  yo <- (xo + eps[,2])*(ys > 0)
@
We use \pkg{mvtnorm} in order to create bivariate normal
disturbances with correlation $-0.7$.  Next, we generate a uniformly
distributed explanatory variable for the selection process, \code{xs}
and the selection outcome by a probit data generating process.
Note that the explanatory variables for the selection process
(\code{xs}) and outcome process (\code{xo}) are different and hence
we have an exclusion restriction.  This can be seen from the fact that
the estimates are quite precise:
<<code:t2summary>>=
  summary(selection(ys~xs, yo ~xo))
@ 
One can see, that all the true values are within the confidence
intervals of the corresponding estimates.

Now we repeat the same exercise, but without exclusion restriction:
<<>>=
yo <- (xs + eps[,2])*(ys > 0)
print(summary(selection(ys ~ xs, yo ~ xs)))
@ 
We can see, that the standard errors are substantially larger in this
case.  The exclusion restriction --- information about the selection
process --- has a certain identifying power which we have lost now.  We
are solely relying on the functional form identification. % Note that
% the true parameters of the selection process are not in the 95\%
% confidence intervals of the estimates any more.
%% ARNE: I tried some different (random) seeds, but the true parameters
%% were always in the 95\% confidence intervals.
%% Therefore, I removed this sentence.

However, we can improve the identification if the selection process
has a lot of explanatory power.  Change the support of \code{xs} from
$[0,1]$ to $[-5,5]$:
<<>>=
  xs <- runif(500, -5, 5)
  ys <- xs + eps[,1] > 0
  yo <- (xs + eps[,2])*(ys > 0)
  print(summary(selection(ys ~ xs, yo ~ xs)))
@ 
Now all the parameters are precisely estimated, even more precisely as
in the case of exclusion restriction.  The reason is simple: the
probability that $Y^O$ is observable, given $x^S$, is $\Pr(X^S + E^O >
0) = 1 - \Phi(-x^S)$ as $E^O \sim N(0,1)$.  As one can see from
Figure~\ref{fig:pnorm}, selection is not an issue if $x^S > 2$.  In
that case $\E [\varepsilon^O|y_i^S = 1] \approx 0$ and hence the
model can be precisely estimated for relatively large values of $x^S$.
Due to the linearity assumption, the same parameters are valid
everywhere.  In the former case, the variation in $x^S$ was low and
hence one could not disentangle the selection bias from the true
causal dependence.  Technically, there was a multicollinearity problem
between $x$ and $\lambda(\cdot)$.

\begin{figure}[htbp]
  \centering
<<fig=TRUE,height=4,echo=FALSE>>=
curve(pnorm, -5, 5, ylab=expression(1 - Phi(-x^S)), xlab=expression(x^S))
@ 
  \caption{Probability that $y^O$ is observable depending on the value
    of $x^S$ (given the standard normal disturbances).}
  \label{fig:pnorm}
\end{figure}



\subsection{Tobit-5 models}
\label{sec:tobit5}

Now let us look at the tobit-5 examples.  This type of problems arises
in a wide variety of contexts, e.g.\ in treatment effect or schooling
choice analysis.  The problem can be written as
\begin{align}
  y_i^{S*} &= \beta^S x_i^S + \varepsilon_i^S
  \\
  y_i^{O1*} &= \beta^{O1} x_i^O + \varepsilon_i^{O1}
  \\
  y_i^{O2*} &= \beta^{O2} x_i^O + \varepsilon_i^{O2}.
\end{align}
We observe
\begin{align}
  y_i^S 
  &= 
  \begin{cases}
    0 & \quad \text{if } y_i^{S*} < 0
    \\
    1 & \quad \text{otherwise}
  \end{cases}
  \\
  y_i^O
  &= 
  \begin{cases}
    y_i^{O1*} & \quad \text{if} y_i^{S} = 0\\
    y_i^{O2*} & \quad \text{otherwise},
  \end{cases}
\end{align}
We assume the disturbance terms have a 3-dimensional normal
distribution:
\begin{equation}
  \begin{pmatrix}
    \varepsilon^S\\
    \varepsilon^{O1}\\
    \varepsilon^{O2}
  \end{pmatrix}
  \sim
  N\left(
    \begin{pmatrix}
      0\\
      0\\
      0
    \end{pmatrix},
    \begin{pmatrix}
      1                 & \varrho_1\sigma_1 & \varrho_2\sigma_2 \\
      \varrho_1\sigma_1 & \sigma_1^2        & \varrho_{12}\sigma_1\sigma_2\\
      \varrho_2\sigma_2 & \varrho_{12}\sigma_1\sigma_2 & \sigma_2^2
    \end{pmatrix}
  \right).
\end{equation}
Note, that $\varrho_2$ plays no role in this model, the observable
distributions are determined by the correlation between the
disturbances of the selection equation ($\varepsilon^S$) and the
corresponding outcome equation ($\varepsilon^{O1}$ and
$\varepsilon^{O2}$).  

We create a simple switching regression problem:
<<>>=
  set.seed(0)
  vc <- diag(3)
  vc[lower.tri(vc)] <- c(0.9, 0.5, 0.1)
  vc[upper.tri(vc)] <- vc[lower.tri(vc)]
  eps <- rmvnorm(500, rep(0, 3), vc)
  xs <- runif(500)
  ys <- xs + eps[,1] > 0
  xo1 <- runif(500)
  yo1 <- xo1 + eps[,2]
  xo2 <- runif(500)
  yo2 <- xo2 + eps[,3]
@ 
We generate a 3-dimensional disturbance vector by \code{rmvnorm}.  We
set the correlation between $\varepsilon^S$ and $\varepsilon^{O1}$ to
be 0.9 and between $\varepsilon^S$ and $\varepsilon^{O2}$ to 0.5.  The
third correlation, 0.1, takes care for the positive definiteness of
the covariance matrix and does not affect the results.  Further, we
create three independent explanatory variables (\code{xs}, \code{xo1}
and \code{xo2}, and hence exclusion restriction is fulfilled).  Note
that we do not have to remove the unobserved values from outcome
vectors, those are simply ignored.  The results look as follows:
<<>>=
  summary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2)))
@ 
We can see, that the parameters are fairly well estimated.  All the
estimates are close to the true values.

Next, take an example of functional form misspecification.  We create
the disturbances as 3-variate $\chi_1^2$ random variables (we subtract
1 in order to get the mean zero disturbances):
<<>>=  
set.seed(5)
eps <- rmvnorm(1000, rep(0, 3), vc)
eps <- eps^2 - 1
xs <- runif(1000, -1, 0)
ys <- xs + eps[,1] > 0
xo1 <- runif(1000)
yo1 <- xo1 + eps[,2]
xo2 <- runif(1000)
yo2 <- xo2 + eps[,3]

summary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2)))
@ 
Although we still have an exclusion restriction,
we can see a substantial bias --- most intercepts are
statistically significant, whereas the true values are zero.  The
model has serious convergence problems, often it does not converge at
all (like in this case, where we use \code{set.seed(5)}).

As the last tobit example, we repeat the previous model without the
exclusion restriction:
<<code:t5chi_woER>>=
set.seed(6)
xs <- runif(1000, -1, 1)
  ys <- xs + eps[,1] > 0
  yo1 <- xs + eps[,2]
  yo2 <- xs + eps[,3]

summary(selection(ys~xs, list(yo1 ~ xs, yo2 ~ xs), iterlim=50))
@
<<code:t5chi_woER_calc, echo=FALSE, results=hide>>=
tmp <- selection(ys~xs, list(yo1 ~ xs, yo2 ~ xs), iterlim=50)
@
%
In most cases, this model does not converge.  However, if it does
(like in this case, where we use \code{set.seed(6)}),
the results may be seriously biased.
Note that first outcome parameters have low standard errors, but a
serious bias.  We present the graph of the correct control function
(based on the $\chi^2$ distribution),
and one where we assume the normal distribution of the disturbance
terms in Figure~\ref{fig:cfunctions}.  We use the estimated parameters
for constructing the latter, however, we scale the normal control
functions (inverse Mill's ratios) to a roughly similar scale, as the
correct ones.
\begin{figure}[htbp]
  \centering
<<fig=TRUE,height=4,echo=FALSE>>=
   EUlower <- function(alpha) {
      alpha[alpha >= 1] <- NA
      alpha <- sqrt(-alpha + 1)
      EUalpha <- ss^2 - 2*ss*alpha*dnorm(alpha/ss)/(1 - 2*pnorm(-alpha/ss))
      s1s^2/ss^2*EUalpha + s1^2 - s1s^2/ss^2 - 1
   }
   EUupper <- function(alpha) {
      alpha[alpha >= 1] <- 1
      alpha <- sqrt(-alpha + 1)
      EUalpha <- (ss*alpha*dnorm(alpha/ss) + ss^2*(1 - pnorm(alpha/ss)))/(1 - pnorm(alpha/ss))
      s2s^2/ss^2*EUalpha + s2^2 - s2s^2/ss^2 - 1
   }
   Nlower <- function(alpha) {
      alpha <- -alpha
      -Ns1s*dnorm(alpha)/pnorm(alpha)
   }
   Nupper <- function(alpha) {
      alpha <- -alpha
      Ns2s*dnorm(-alpha)/pnorm(-alpha)
   }
   ss <- sqrt(vc[1,1])
   s1 <- sqrt(vc[2,2])
   s2 <- sqrt(vc[3,3])
   s1s <- vc[1,2]
   s2s <- vc[1,3]
   Ns1s <- coef(tmp)["sigma1"]*coef(tmp)["rho1"]
   Ns2s <- coef(tmp)["sigma2"]*coef(tmp)["rho2"]
hatb1O <- coef(tmp)[c("XO1(Intercept)", "XO1xs")]
hatb2O <- coef(tmp)[c("XO2(Intercept)", "XO2xs")]
   es <- eps[,1]
   e1 <- eps[,2]
   e2 <- eps[,3]
   ex <- seq(-5, 5, length=200)
ey <- cbind(EUlower(ex), EUupper(ex),
            -31*Nupper(cbind(1, ex)%*%hatb1O), 5.5*Nlower(cbind(1,ex)%*%hatb2O)
###         -s1s*dnorm(-ex)/pnorm(-ex), s2s*dnorm(ex)/pnorm(ex)
            )
   mcy <- matrix(0, length(ex), 2)
   for(i in seq(length=nrow(mcy))) {
      mcy[i,1] <- mean(e1[es < -ex[i]])
      mcy[i,2] <- mean(e2[es > -ex[i]])
   }
par(cex=0.8, mar=c(3,3,0,0) + 0.1, mgp=c(2,1,0))
   matplot(ex, ey, type="l", lty=c(1,1,2,2,3,3), col=1,
           xlab=expression(x^S), ylab="",
           ylim=c(-1.5,2.5))
abline(v=-1,lty=3)
abline(v=1, lty=3)
#   matpoints(ex, mcy, pch=c(1,2), cex=0.5, col=1)
   axis(4)
text(-3.5, 0.8, 
     expression(paste("correct  ")*E*group("[", Epsilon^{O1}*group("|", Epsilon^S > -x^S, ""), "]")))
text(-3.6, -0.4,
     expression(paste("correct  ")*E*group("[", Epsilon^{O1}*group("|", Epsilon^S < -x^S, ""), "]")))
text(-0.7, 1.5,
     expression(E*group("[", Epsilon^{O1}*group("|", 
         Epsilon^S > -bold(beta)^S*minute*bold(x)^S, ""), "]")*
         paste("  assumed normal")))
text(2.2, -1.4,
     expression(E*group("[", Epsilon^{O1}*group("|", 
         Epsilon^S < -bold(beta)^S*minute*bold(x)^S, ""), "]")*
         paste("  assumed normal")))
@ 
  \medskip \\
  \small Note:
  Vertical lines denote the support of $x^S$ in the MC experiment.
  \caption{Control functions: correct ($\chi^2$ distributed)
    and assumed (normally distributed)}
  \label{fig:cfunctions}
\end{figure}
One can see, that the functions differ substantially in the relevant
range of $x^S \in [-1, 1]$.  In particular, the upper mean rises
substantially faster close to $x^S = 1$ than the normal
approximation.  The correct lower mean is decreasing slightly faster
than the approximation.

It is instructive to estimate the same model as a two independent OLS
equations:
<<>>=
  summary(lm(yo1~xs, subset=ys==0))
  summary(lm(yo2~xs, subset=ys==1))
@ 
%
One can see that the OLS estimates are very close to the ML ones, because
none of the $\varrho$s is statistically significant.




\section{Additional notes}
\label{sec:additional}

The log-likelihood of the models above is not globally concave.  It
means, that the model may converge to different estimates (or not
converge at all), if the initial values are not chosen well enough.
We illustrate this behaviour on real individual data.  Consider the
following example:
<<init_2step>>=
data(Mroz87)
mr <- selection(lfp ~ kids5 + poly(age, 2) + educ + log(huswage) + mtr + fatheduc + city + unem,
log(hours) ~ kids5 + poly(age, 2) + educ + wage + mtr + city + poly(exper, 2),
data=Mroz87)
@ 
%
We use a supplied dataset, \code{Mroz87}, used for analysing the
female labour supply by \citet{mroz1987}.  We model the labour force
participation (described by dummy \code{lfp}) by presence of young
children (\code{kids5}), quadratic polynomial in age, education in
years (\code{educ}), log husbands wage (\code{huswage}), marginal tax
rate (\code{mtr}), father education (in years, \code{fatheduc}),
residence in a big city (\code{city}) and the local unemployment rate
(\code{unem}).  The labour supply equation includes most of the
variables above (leaving \code{huswage}, \code{fathedu} and
\code{unem} for exclusion restrictions).  The new explanatory
variables are woman's wage (\code{wage}) and quadratic polynomial in
experience (\code{exper}).

The model gives reasonable results:
<<init_0>>=
summary(mr)
@ 
%
One may see, that small children in the family, high salary of the
husband and high marginal tax rate decrease the participation
probability, while education and residence in a big city increase it.
The labour supply equation seems a bit less convincing with more
education and income leading to less hours worked and small kids
causing women to work more.  The correlation of error terms \code{rho}
shows a nearly perfect negative correlation between the corresponding
error terms, which means we estimate essentially a tobit model (see
\citet{mroz1987} for more discussion).

However, the results are sensitive to the start values
(note that you have to supply a positive initial value for the
variance):
<<>>=
mr <- selection(lfp ~ kids5 + poly(age, 2) + educ + log(huswage) + mtr + fatheduc + city + unem,
log(hours) ~ kids5 + poly(age, 2) + educ + wage + mtr + city + poly(exper, 2),
data=Mroz87, start=c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0))
summary(mr)
@ 
%
The maximisation routine gets stuck in a local feature and
cannot find the global maximum.  Here another optimisation algorithm
might be useful, however, switch of the algorithm is not yet
implemented.  In far the most cases, the 2-step method, used for
calculating the initial values, does a good job.

In general, one should prefer \code{method="ml"} instead of
\code{"2step"}.  However, ML may have problems at the boundary of the
parameter space.  Take a textbook tobit example: 
\begin{equation}
  \label{eq:tobit}
  y_i^* = \vec{\beta}' \vec{x}_i + \varepsilon_i;
  \quad
  y_i =
  \begin{cases}
    y_i^* \quad & \text{if } y_i^* > 0\\
    0           & \text{otherwise.}
  \end{cases}
\end{equation}
This model can be written as a tobit-2 model where the error term of
the selection and outcome equation are perfectly correlated.  In this
case the ML may not converge:
<<tobit_tobit2>>=
set.seed(0)
x <- runif(1000)
y <- x + rnorm(1000)
ys <- y > 0
summary(selection(ys~x, y~x))
@ 
%
However, the 2-step method still works:
<<>>=
summary(selection(ys~x, y~x, method="2step"))
@ 
%



\section{Conclusions}

This paper describes the Heckman-type selection models and their
implementation in \pkg{micEcon} for programming language \proglang{R}.
We argue, that although there exist superior estimation methods for
analysing censored data, these models still serve as useful teaching
tools as they are easy to implement and understand.

We briefly describe the implementation and usage, and demonstrate the
virtues and caveats of these models using a number of examples.  Our
implementation works well for correctly specified cases, in the case
of severe misspecification a more robust maximisation algorithm might
be useful.  This is left for the future work.



\bibliography{selection}

\end{document}
