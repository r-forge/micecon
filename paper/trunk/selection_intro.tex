
Social scientists are often interested in causal effects -- what is
the impact of a new drug, a certain type of school or eg being born as
a twin.  Many of these cases are under our (partial) control.  In most
cases, we can decide ourselves, whether we take the drug or which
school we attend.  We cannot control whether we are twins, but neither
can the reasearcher -- if the twins tend to be born in a different
types of families, the researcher never has all the relevant
information in order to control for the difference.

This problem -- people who are ``treated'' are different that the rest
of the population -- is usually refered to as the \emph{sample
  selection} problem.  We cannot estimate the causal effect, unless we
solve the selection problem.  Otherwise, we never know which part of
the outcome is related to the fact that different people were selected
to the treatment and control groups, and which part is due to the
causal relationship.


\subsection{Possible solutions}

All the solutions involve using additional information.  There are
several possibilites which may or may not be useful for any particular
case. 

\begin{itemize}
\item Randomisation: experiment, where the participants \emph{do not}
  have control over their treatment status.  Although easy to analyse,
  this way is seldom feasible for pracitcal and ethical reasons.
\item Instruments (exclusion restrictions): more-or-less
  randomisation.  Just not explicit, based on background, institutions
  etc.
\item Information about the functional form.  For instance,
  assumptions about the distribution of the error terms.
\item Timing information.  In some cases, relative timing of the
  events may give us information, necessary for identification of the
  causal effect.
\end{itemize}

During the recent decades, either randomisation or the
pseudo-randimisation (natural experiments) has become the state-of-the
art while estimating the causal effects.  The methods, relying on the
distributional assumptions are becoming less widely used.  The reason
is obvious -- the parametric assumptions can only seldom be justified,
and we don't want our results to rely on dubious assumptions.  Even if
we have valid instruments or exclusion restrictions, the parametric
hypothesis add their interference which may be sometimes hard to
detect.  

However, even if these models are not popular in research any more,
they still serve as excellent teaching tools.  Heckman-type selection
models easily allow us to experiment with selection bias,
misspecification, exclusion restrictions etc.  These models are easy
to visualise and understand.


\subsection{Heckman models}

\citet{heckman76} proposed a two-step solution to the fully-parametric
model.  He assumed bivariate normal error distribution.

The original article suggests using the (less efficient) two-step
solution for exploratory work and as the initial value for maximum
likelihood estimation -- two-step solution costs \$15 while the
maximum-likelihood solution costs \$700.

